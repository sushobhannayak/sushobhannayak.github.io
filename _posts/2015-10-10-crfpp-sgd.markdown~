---
layout: post
title:  "On my (Stochastic Gradient) Descent into CRF++"
date:   2015-10-10 20:31:00
categories: sgd ml crf
---

>"Begin at the beginning," the King said, very gravely, "and go on till you come to the end: then stop."

I was recently reading this [awesome paper][shewchuk-cg] on conjugade gradients (and I think you should too, 
if you are even barely interested in figuring out what goes on at the heart of all those 
weight optimization routines), which led me to delve deeper into other optimization algorithms
and investigate which of them are considered suitable for learning models from large-scale data.
On one hand we have batch methods like gradient descent, L-BFGS, and conjugate gradient descent, with
their well-studied and theoretically guaranteed faster convergence rates, while on the other,
we have approximations like stochastic gradient descent (SGD), with their poorer
convergence rates. But surprisingly, it turns out
that SGD is the clear winner ([[1]][lecun][[2]][bengio][[3]][botou]), performing miraculously well 
in large neural networks and on massive data. Now it so happened that I recently got a little tired
of watching my [CRF++][crfpp] program, which uses fast LBFGS optimization and has been a constant
loyal companion in all my tagging expeditions over the past year, doggedly crunching bits 
for 24 hours on my laptop, trying to optimize just 100k tagged samples, and I thought, well, it's
high time I did some hacking around and put the theory to test.
While I was familiar with most of the above mentioned methods, especially so with SGD, it was a classic case of 
'knowing the name of something vs knowing something', most of my attempts being relegated to 
using them as off-the-shelf black-boxes or squinty-eyed yawn-punctuated acrobatics 
on a keyboard to cook up a 5-line matlab function to plot some graphs at 2 in the night to meet the
next morning assignment deadline, with hardly any insight into their inner-workings or caveats. 
This post delineates my attempts at integrating SGD and related methods with CRF++ and reports 
the insights I gained thereof. The git repo can be found [here][git].

##Down the rabbit hole
Consider the usual supervised learning setup, where we have a set of training examples $$(x_i,y_i)$$
and we are trying to minimize the following cost function

$$
E_n(w) = \frac{\lambda}{2}\|w\|^2 + \frac{1}{n}\sum_{i=1}^n L(x_i,y_i,w)
$$

the $$w$$ that minimizes it being weights we are interested in. Since it's hard/impossible to 
directly solve the equation, optimization methods usually employ iterative metods, 
starting with an intial guess $$w_0$$ for $$w$$, iterating through the training examples and then updating it 
to $$w_t$$ get a lower $$E_n(w_t)$$ on the next iteration, doing this till we are unable to make it any
smaller. Gradient descent utilizes the notion that the gradient of $$E_n$$ at any point $$w_t$$ is the 
direction of steepest increase of $$E_n$$ and so, if we want to find the minimum $$E_n$$, we had better
move in the directly opposite direction, resulting in an updation rule like this:

$$
w_{t+1} = w_t - \gamma \big(\lambda w_t + \frac{1}{n}\sum_i L'(x_i, y_i, w_t)\big)
$$

where $$\gamma$$ is the learning rate (more on this later). LBFGS essentially replaces constant $$\gamma$$
with matrix $$\Gamma_t$$ which approaches the inverse of the Hessian of $$E_n$$, the particulars of which are
explained in more detail than I would be able to explain [here][aria-post]. This usually leads to faster
convergence than pure gradient descent since the Hessian is a measure of the curvature of $$E_n$$, and it leads
to a better descent direction. If the initial weight $$w_0$$ is close enough to the optimum, and the learning rate
$$\gamma$$ is sufficiently small, the above methods lead to an exponential asymptotic decrease in the cost function.
(We will see some interesting consequences of the assumption of the closeness of $$w_0$$ to $$w_{opt}$$ later)

The SGD is a simplified approximation of gradient descent(GD). It says, hey look, all the examples are sampled 
from the same original distribution any way, so why not estimate the gradient on the basis of a single randomly
picked sample from the dataset, which leads to an updation rule like the following:

$$
w_{t+1} = w_t - \gamma \big( \frac{\lambda}{n} w_t + L'(x_t, y_t, w_t)\big)
$$

So here's the question, if it's just an approximation of the gradient of $$E_n$$, how come it has faster convergence
rate than GD, which works on the true empirical gradient, and works better on massive datasets? Well, it doesn't. Its 
convergence speed is actually limited by the noisy approximation of the true gradient, and is asymptotically much
slower than GD (it takes $$O(1/\rho)$$ time compared to $$O(n \log 1/\rho)$$ for GD and $$O(n\log \log 1/\rho)$$ for LBFGS 
to reach within $$\rho$$ bound of $$E_{n,min}$$). However, what it does really well is to reach a predefined expected
risk faster than the others. When we set out saying that we are trying to minimize $$E_n$$, we actually lied a bit. What we 
really wanted to minimize was $$E$$, the expected risk, which is independent of $$n$$. $$E_n$$ is just a proxy for $$E$$,
since we don't know the correct distribution and only have $$(x_i, y_i)$$ pairs as samples from the original distribution. 
SGD is pretty good at getting to a predefined $$E$$ faster, because it can process more training examples in the alloted
time, which reduces the variance. Section 3 of [this paper][botou] gives a nice derivation of the bounds and convergence 
rates. Intuitively, it boils down to the following: it makes use of the redundancy in the dataset. Similar clusters of
pattern appear in large datasets; a batch algorithm only calculates the gradient after seeing them all. SGD, on the other hand,
can get the same average gradient over that cluster on seeing it once: so a lot of time is spent calculating gradients in GD
and less time in updating weights, while it's the other way round with SGD. As an added bonus, since updates in SGD are noisy,
they can sometimes take you to another basin in the contour, with perhaps better a minima, while batch methods are stuck
in the basin they were initialized in. 

#She drinks the potion of reality
So I hacked the CRF++ codebase to set up an SGD routine, that shuffles the data on each epoch (one full iteration through the dataset),
and updates the weights after seeing each example. I use the datasets provided with the original CRF++ suite, which are really small,
with around 100 odd sentences in each, and threw in the 9000 sentences each from [conll2000 chunking dataset][conll-data] and a private
dataset for good measure. I used a learning rate of 0.01, and $$\lambda$$ of 10, and discarded features that occured less than 3 times. 
And I also printed out the current error and cost statistics by sampling every 5 epochs, 
to make sure that optimization was proceeding correctly. The numbers quoted below are testing accuracies on the test sets of the corresponding
datasets, and the time is noted in seconds in brackets. The original CRF++ is running LBFGS in 8 threads, while SGD is single-threaded.

| Dataset/*Epochs*|*5(LBFGS)*|*10(LBFGS)*|*100(LBFGS)*| *To Conv(LBFGS)* | *5(SGD)*|*10(SGD)*| *100(SGD)* |
| ------------- |:-------------:| :-----:|:-----:|:-----:|:-----:|:---------:|:-------:|
|*JapaneseNER*| **92.89**(1.18) | **93.04**(2.33) | **96.17**(14.20) | **96.17**(14.20) | **95.20**(1.50) | **95.84**(2.91) | **96.08**(28.82)|
|*conll2000*| **87.35**(87.34) | **92.77**(186.35) | **95.76**(1846.19) |**95.88**(4044.17)| **95.54**(416.18) | **95.78**(834.71) |**95.99**(8152.78)
|*chunking* | **86.01**(0.33) | **89.23**(0.63) | **89.85**(2.78) | **89.85**(2.78) | **85.89**(0.32) | **87.67**(0.64) | **89.91**(5.84)
|*other*| **36.88**(19.55)|**42.25**(39.05)|**68.41**(413.30)|**69.23**(5289.52)| **53.89**(108.48) | **50.09**(217.85)|**69.44**(2137.49)

Each epoch is one iteration through the dataset. The numbers above track our discussion fabulously. SGD gets to a pretty good test accuracy after
just 5 epochs on both datasets, while LBFGS perspires on for 10 to 100 iterations on the datasets, the difference being more marked as we 
move to a larger dataset. Consequently, even though individual epochs take longer in SGD (they naturally will, since we are updating weights after
seeing every example; so we are updating feature number of variables every time, and feature vector cardinality can be really huge in such 
tasks, it is 1679700 for conll dataset for example), we get to a decent region quite soon. This is the getting a quite low expected risk
$$E$$ part. As we further increase the number of epochs, we however have limited further improvement, with we seemingly spending a lot of time 
doing a meagre improvement in accuracy (in both cases, improvement in accuracy is $$< 1\%$$ after initial 5 iterations), which is the 
it-takes-too-late-to-get-to-the-absolute-minimum part; but fortunately, most of the time, 
the absolute minimum really doesn't matter -- we are pretty happy getting to a close enough vicinity of the minimum. But that doesn't seem to be much
of a concern for this experiment, as SGD is in fact consistently out-performing LBFGS, and getting us to a better minima. One thing to notice though is 
the drop in accuracy from 54% to 50% in the final dataset after running the algorithm for 5 more epochs. We will come to that next, and see how learning
rate can either make or break your day.

#Where is my cat
In SGD, since the estimation of the gradient is noisy, the learning rate $$\gamma$$ plays a really prominent role in convergence of the routine. 
Make it too large, and you move the weight around irratically, bouncing it off the side of the basin, at times converging towards the bottom, and 
aiming for the sky on the next iteration. Make it too small, and it can't take a big enough step, thus giving into traipsing around instead of homing in on the 
minima. Ideally, it should be high in the beginning, where we want the optimization routine to explore more and take large noisy steps, and gradually come down,
approaching zero as we get closer to the minima, so reduce severe fluctuations in an already stable weight. Theoretically, the decreasing learning rate $$\gamma_t$$
needs to follow the following properties for convergence: $$\sum_t \gamma_t  = \infty, \sum_t \gamma_t^2 < \infty$$. In practice, usually a learning rate of the
form $$\gamma_t = \gamma_0 (1+\gamma_0\lambda t)^{-1}$$ works well, where $$\gamma_0$$ is the initial $$\gamma$$ at time $$t=0$$ and $$\lambda$$ is the regularization 
parameter we have been using. In the experiments quoted above, I used a single $$\gamma_0 = 0.01$$ for all the datasets. This is not a very good idea, since
each dataset is different, each cost space disparate, with individual countours and shapes, which a second order method like LBFGS can leverage since it
approximates that information in the quasi-Hessian it calculates on each iteration: but SGD would be at a loss, with an uninspired learning rate leading to
slow convergence or zig-zagging. So how do we decide on a good learning rate for each dataset? How about we SGD again and again changing $$\gamma$$ each time and 
picking up the model that lead to the lowest validation error? That's topping, except when you are looking for a fast turn around rate, it essentially necessitates 
running multiple instances of the routine on different machines parallely. So what do we do? Well, let's approximate: SGD approximates the gradient; we can 
approximate multiple complete runs with different $$\gamma$$s, by running multiple single epochs with different $$\gamma$$s on a sample of the whole dataset, 
and picking up the best $$\gamma$$ from those runs (which can be efficiently done with a binary search, half the next $$\gamma$$ to try if the present one is too
small, double it if it's the contrary, which is the approach followed by [3][botou] -- see their sgd [code][botou-sgd]). That's what I did, and here are the updated
results for the two big datasets:

|Dataset/*Epoch* |$$\gamma_0$$| *5(SGD)* | *10(SGD)* |
|:----------------|:-------:|----------:|---------:|
|*conll2000* | 0.05 | **95.84**(527.19) | **95.87**(949.94)|
|*other* |0.00078|**60.58**(204.87) | **68.16**(274.75)|

Notice that these values converge much faster to the minimum compared to the previous run with an arbitrary learning rate, at the expense of running a few more epochs(10 to be specific).
Since the previous $$\gamma_0$$($$0.01$$) for *conll2000* was pretty close to the optimal($$0.05$$), there is no glaring difference; but it's evident that we take larger steps in the 
latter that leads to a better weight vector for the same number of epochs. It's the opposite with *other*, where the former $$\gamma_0$$ of $$0.01$$ was significantly larger than
the optimal value of $$0.00078$$; so I guess the weight updations zigzagged a lot around the valley, diverging at times, definitely between epochs 5 and 10, as evidenced by the 
drop in accuracy on the test set from $$53\%$$ to $$50\%$$ after 5 and 10 iterations. The second round of runs, in strong contrast, gives significantly better numbers on the test set.
In the output log of the program, we can clearly see the objective running haywire for the first round of tests:

{% highlight ruby %}
 terr=0.62838 serr=0.75921 obj=9.17097 diff=1.00000
epoch =0 weight = 2863.55448
 terr=0.39495 serr=0.57655 obj=5.17678 diff=0.43553
epoch =1 weight = 4440.64808
 terr=0.48286 serr=0.63081 obj=6.90334 diff=0.33352
epoch =2 weight = 5757.25287
 terr=0.43083 serr=0.61483 obj=7.08558 diff=0.02640
epoch =3 weight = 6951.25845
 terr=0.39612 serr=0.53246 obj=5.18373 diff=0.26841
...
 terr=0.36398 serr=0.50242 obj=5.24929 diff=1.96482
epoch =26 weight = 22490.62973
 terr=0.18194 serr=0.31928 obj=1.93873 diff=0.63067
epoch =27 weight = 22902.92107
 terr=0.34560 serr=0.46221 obj=4.46865 diff=1.30494
...
{% endhighlight %}

Okay, so we started out trying to incorporate SGD routines into CRF++, to make it faster, and after bouncing around some theoretical and practical considerations and toiling a bit to get 
our wayward learning rate home, it seems we are finally there. Time to bring out the hammock and sip some pina colada! Or is it?

##Welcome to the mad tea-party
My laptop has 8 cpu cores. While one of them is madly spewing out gradients, the other seven are dozing off like the Dormouse. SGD is inherently sequential: even though it beats
the LBFGS routine running on all 8 cores, wouldn't it be nicer still if we can get those idling Dormouses to partake in our tea-party? Wake up those lazy bums and pass each a cup of hot simmering tea:
Alas! we have got just one tea-pot, and now the Dormouses are clawing at each other to get hold of it. In case the allegory got too far fetched, it was simply pointing out that parallelizing SGD
is difficult, since the over-head involved with acquiring/releasing locks on the weight vector quickly nullifies any advantages of the fast routine. So what do we do? Time to go [Hogwild][hogwild] !










[shewchuk-cg]:	    https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf
[aria-post]:	    http://aria42.com/blog/2014/12/understanding-lbfgs/
[conll-data]:	    http://www.clips.uantwerpen.be/conll2000/chunking/
[lecun]:	    http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf
[bengio]:	    http://arxiv.org/abs/1206.5533
[botou]:	    http://research.microsoft.com/pubs/192769/tricks-2012.pdf
[git]:		    https://github.com/sushobhannayak/crfpp-extensions
[crfpp]:	    https://taku910.github.io/crfpp/
[botou-sgd]:	    https://github.com/npinto/bottou-sgd/blob/master/crf/crfsgd.cpp#L1562
[hogwild]:	    https://www.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf